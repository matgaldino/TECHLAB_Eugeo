{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "GOOGLE_API_KEY= os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolher as funções disponíveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Encaminhe uma consulta de usuário para o datasource mais relevante.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"websearch\", \"agendamento\"] = Field(\n",
    "        ...,\n",
    "        description=\"Dada uma pergunta do usuário, escolha encaminhá-la para websearch, vectorstore ou agendamento.\",\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"Você trabalha na Tech4.ai e é um especialista em encaminhar uma pergunta do usuário para um vectorstore, pesquisa na web ou agendamento de reunião. \\n\n",
    "O vectorstore contém documentos relacionados a perguntas comuns apenas sobre a empresa (Tech4.ai), tais como missão, visão, valores, cultura, programas internos, políticas \\n\n",
    "de trabalho remoto, horários, etc. Use o vectorstore para perguntas sobre esses tópicos. \\n\n",
    "Para perguntas sobre as ferramentas Github, Vscode, Jira e discord use a pesquisa na web. \\n\n",
    "Para perguntas sobre agendamento de reuniões use o agendamento.\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n",
      "datasource='vectorstore'\n",
      "datasource='vectorstore'\n",
      "datasource='agendamento'\n",
      "datasource='agendamento'\n",
      "datasource='websearch'\n",
      "datasource='websearch'\n",
      "datasource='websearch'\n"
     ]
    }
   ],
   "source": [
    "print(question_router.invoke({\"question\": \"Qual a missão da Tech4.ai?\"}))\n",
    "print(question_router.invoke({\"question\": \"Quero saber os valores da empresa\"}))\n",
    "print(question_router.invoke({\"question\": \"Quero saber o que esperar de um dia de trabalho na Tech4.ai\"}))\n",
    "print(question_router.invoke({\"question\": \"gostaria de agendar uma reunião com o time de desenvolvimento\"}))\n",
    "print(question_router.invoke({\"question\": \"Como posso marcar uma reunião com o time de desenvolvimento?\"}))\n",
    "print(question_router.invoke({\"question\": \"Preciso de ajuda com o Github\"}))\n",
    "print(question_router.invoke({\"question\": \"Como posso usar o Vscode?\"}))\n",
    "print(question_router.invoke({\"question\": \"Como posso usar o Jira?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# Load PDF document\n",
    "pdf_path = \"Base.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the combined documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "\"\"\" \n",
    "(VOLTAR SE SOBRAR TEMPO!)\n",
    "O text_splitter não pega as imagens no final do pdf (Valores da Empresa). Tentativa de extrair texto das imagens:\n",
    "# Extract text from images\n",
    "image_paths = [\"image1.PNG\", \"image2.PNG\", \"image3.PNG\", \"image4.PNG\", \"image5.PNG\", \"image6.PNG\"]\n",
    "image_texts = [extract_text_from_image(image_path) for image_path in image_paths]\n",
    "\n",
    "doc_splitsIMAGES = text_splitter.split_documents(text_splitter.create_documents(image_texts)) \n",
    "\"\"\"\n",
    "\n",
    "# Load an open-source embedding model from Hugging Face\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Add to vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"Empresa\",\n",
    "    embedding=embedding_function,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Usar para deletar vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore = Chroma(collection_name=\"Empresa\")\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    " \n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Pontuação binária para verificar a relevância nos documentos utilizados.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documentos são relevantes para a pergunta, 'sim' or 'não'\")\n",
    "\n",
    "# LLM with function call \n",
    "structured_llm_grader_docs = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"Você é um avaliador que avalia a relevância de um documento recuperado para uma pergunta do usuário. \\n\n",
    " Se o documento contiver palavra(s)-chave ou significado semântico relacionado à pergunta, classifique-o como relevante. \\n\n",
    " Dê uma pontuação binária \"sim\" ou \"não\" para indicar se o documento é relevante para a pergunta.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader_relevance = grade_prompt | structured_llm_grader_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Você é um assistente para tarefas de resposta a perguntas. Use as seguintes partes do contexto recuperado para responder à pergunta. \\n\n",
    "      Se você não souber a resposta, apenas diga que não sabe. Sempre responda em português do Brasil.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    ")\n",
    " \n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='sim'\n",
      "Os programas internos da empresa mencionados são:\n",
      "\n",
      "1. Star Performers: reconhece os funcionários de alto desempenho na empresa.\n",
      "2. Monthly Highlights: destaca as atividades realizadas por cada departamento no mês passado.\n",
      "3. Team Talk: é um podcast apresentado por funcionários da empresa, discutindo tópicos relevantes e compartilhando insights.\n",
      "4. Network Sessions: promove o networking entre os funcionários em torno de temas pré-definidos.\n",
      "5. Learning Moments: apresenta palestras e workshops com especialistas externos para promover o aprendizado contínuo.\n",
      "6. Book Club: é um programa de leitura onde os funcionários compartilham conhecimentos e aprendem uns com os outros.\n",
      "7. Talent Development: visa o desenvolvimento comportamental dos funcionários.\n",
      "8. Engagement Projects: são projetos criados para aumentar o engajamento dos funcionários, incluindo jogos e atividades de produtividade.\n",
      "9. Referral Program: oferece uma recompensa aos funcionários que indicam candidatos bem-sucedidos para vagas na empresa.\n",
      "10. Innovation Lab: é um espaço para a criação de projetos que utilizam Inteligência Artificial.\n",
      "11. Education Support: oferece incentivos para cursos, treinamentos e livros.\n",
      "12. Travel Benefits: é um programa de benefícios de viagem que oferece descontos significativos e um orçamento para viagens, pago pela empresa.\n",
      "13. Team Integration: é um evento de integração semestral para todos os times da empresa.\n"
     ]
    }
   ],
   "source": [
    "question = \"Quais são os programas internos da empresa?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader_relevance.invoke({\"question\": question, \"document\": doc_txt}))\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination Grader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='sim')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Pontuação binária para alucinação presente na resposta obtida.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Não considere a possibilidade de chamar APIs externas para obter informações adicionais. A resposta é apoiada pelos fatos, 'sim' ou 'não'.\")\n",
    " \n",
    "# LLM with function call \n",
    "structured_llm_grader_hallucination = llm.with_structured_output(GradeHallucinations)\n",
    " \n",
    "# Prompt \n",
    "system = \"\"\"Você é um avaliador que avalia se uma resposta gerada por LLM é apoiada por um conjunto de fatos recuperados. \\n \n",
    "     Restrinja-se a dar uma pontuação binária, seja \"sim\" ou \"não\". Se a resposta for apoiada ou parcialmente apoiada pelo conjunto de fatos, considere-a um sim. \\n\n",
    "    Não considere a chamada de APIs externas para obter informações adicionais.\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "]\n",
    ")\n",
    "  \n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader_hallucination\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Grader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='sim')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Pontuação binária para avaliar se a resposta responde a pergunta.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Responde responde a pergunta, 'sim' ou 'não'\")\n",
    "\n",
    "# LLM with function call \n",
    "structured_llm_grader_answer = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"Você é um avaliador que avalia se uma resposta aborda/resolve uma pergunta \\n \n",
    "     Dê uma pontuação binária 'sim' ou 'não'. 'Sim' significa que a resposta resolve a pergunta.\"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader_answer\n",
    "answer_grader.invoke({\"question\": question,\"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web-Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os.path\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/calendar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "could not locate runnable browser",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     flow \u001b[38;5;241m=\u001b[39m InstalledAppFlow\u001b[38;5;241m.\u001b[39mfrom_client_secrets_file(\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcredentials.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, SCOPES\n\u001b[1;32m     17\u001b[0m     )\n\u001b[0;32m---> 18\u001b[0m     creds \u001b[38;5;241m=\u001b[39m \u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_local_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mport\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Save the credentials for the next run\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m token:\n",
      "File \u001b[0;32m~/TECHLAB/venv/lib/python3.10/site-packages/google_auth_oauthlib/flow.py:444\u001b[0m, in \u001b[0;36mInstalledAppFlow.run_local_server\u001b[0;34m(self, host, bind_addr, port, authorization_prompt_message, success_message, open_browser, redirect_uri_trailing_slash, timeout_seconds, token_audience, browser, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m auth_url, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauthorization_url(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m open_browser:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# if browser is None it defaults to default browser\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     \u001b[43mwebbrowser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbrowser\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mopen(auth_url, new\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, autoraise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authorization_prompt_message:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28mprint\u001b[39m(authorization_prompt_message\u001b[38;5;241m.\u001b[39mformat(url\u001b[38;5;241m=\u001b[39mauth_url))\n",
      "File \u001b[0;32m/usr/lib/python3.10/webbrowser.py:65\u001b[0m, in \u001b[0;36mget\u001b[0;34m(using)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m command[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m command[\u001b[38;5;241m0\u001b[39m]()\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not locate runnable browser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: could not locate runnable browser"
     ]
    }
   ],
   "source": [
    "\"\"\"Shows basic usage of the Google Calendar API.\n",
    "Prints the start and name of the next 10 events on the user's calendar.\n",
    "\"\"\"\n",
    "creds = None\n",
    "# The file token.json stores the user's access and refresh tokens, and is\n",
    "# created automatically when the authorization flow completes for the first\n",
    "# time.\n",
    "if os.path.exists(\"token.json\"):\n",
    "    creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\n",
    "            \"credentials.json\", SCOPES\n",
    "        )\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open(\"token.json\", \"w\") as token:\n",
    "        token.write(creds.to_json())\n",
    "\n",
    "try:\n",
    "    service = build(\"calendar\", \"v3\", credentials=creds)\n",
    "\n",
    "    # Call the Calendar API\n",
    "    now = datetime.datetime.utcnow().isoformat() + \"Z\"  # 'Z' indicates UTC time\n",
    "    print(\"Getting the upcoming 10 events\")\n",
    "    events_result = (\n",
    "        service.events()\n",
    "        .list(\n",
    "            calendarId=\"primary\",\n",
    "            timeMin=now,\n",
    "            maxResults=10,\n",
    "            singleEvents=True,\n",
    "            orderBy=\"startTime\",\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "    events = events_result.get(\"items\", [])\n",
    "\n",
    "    if not events:\n",
    "        print(\"No upcoming events found.\")\n",
    "\n",
    "    # Prints the start and name of the next 10 events\n",
    "    for event in events:\n",
    "        start = event[\"start\"].get(\"dateTime\", event[\"start\"].get(\"date\"))\n",
    "        print(start, event[\"summary\"])\n",
    "\n",
    "except HttpError as error:\n",
    "    print(f\"An error occurred: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents \n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    web_search : str\n",
    "    documents : List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE from Vector Store DB---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE Answer---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader_relevance.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"sim\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "    \n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH. Append to vector store db---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "### Edges\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG \n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})   \n",
    "    if source.datasource == 'websearch':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source.datasource == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"sim\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"sim\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search) # web search # key: action to do\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # generatae\n",
    "\n",
    "workflow.add_edge(\"websearch\", \"generate\") #start -> end of node\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    " \n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\", # start: node\n",
    "    decide_to_generate, # defined function\n",
    "    {\n",
    "        \"websearch\": \"websearch\", #returns of the function\n",
    "        \"generate\": \"generate\",   #returns of the function\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\", # start: node\n",
    "    grade_generation_v_documents_and_question, # defined function\n",
    "    {\n",
    "        \"not supported\": \"generate\", #returns of the function\n",
    "        \"useful\": END,               #returns of the function\n",
    "        \"not useful\": \"websearch\",   #returns of the function\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE from Vector Store DB---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: grade_documents:'\n",
      "---GENERATE Answer---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "('Os programas internos da empresa mencionados são:\\n'\n",
      " '\\n'\n",
      " '1. Star Performers: reconhece os funcionários de alto desempenho na '\n",
      " 'empresa.\\n'\n",
      " '2. Monthly Highlights: destaca as atividades realizadas por cada '\n",
      " 'departamento no mês passado.\\n'\n",
      " '3. Team Talk: é um podcast apresentado por funcionários da empresa, '\n",
      " 'discutindo tópicos relevantes e compartilhando insights.\\n'\n",
      " '4. Network Sessions: promove o networking entre os funcionários em torno de '\n",
      " 'temas pré-definidos.\\n'\n",
      " '5. Learning Moments: apresenta palestras e workshops com especialistas '\n",
      " 'externos para promover o aprendizado contínuo.\\n'\n",
      " '6. Book Club: é um programa de leitura onde os funcionários compartilham '\n",
      " 'conhecimentos e aprendem uns com os outros.\\n'\n",
      " '7. Talent Development: visa o desenvolvimento comportamental dos '\n",
      " 'funcionários.\\n'\n",
      " '8. Engagement Projects: são projetos criados para aumentar o engajamento dos '\n",
      " 'funcionários, incluindo jogos e atividades de produtividade.\\n'\n",
      " '9. Referral Program: oferece uma recompensa aos funcionários que indicam '\n",
      " 'candidatos bem-sucedidos para vagas na empresa.\\n'\n",
      " '10. Innovation Lab: é um espaço para a criação de projetos que utilizam '\n",
      " 'Inteligência Artificial.\\n'\n",
      " '11. Education Support: oferece incentivos para cursos, treinamentos e '\n",
      " 'livros.\\n'\n",
      " '12. Travel Benefits: é um programa de benefícios de viagem que oferece '\n",
      " 'descontos significativos e um orçamento para viagens, pago pela empresa.\\n'\n",
      " '13. Team Integration: é um evento de integração semestral para todos os '\n",
      " 'times da empresa.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "inputs = {\"question\": \"Quais são os programas internos da empresa?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
