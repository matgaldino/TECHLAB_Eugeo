{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "GOOGLE_API_KEY= os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolher as funções disponíveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Encaminhe uma consulta de usuário para o datasource mais relevante.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"websearch\", \"agendamento\"] = Field(\n",
    "        ...,\n",
    "        description=\"Dada uma pergunta do usuário, escolha encaminhá-la para websearch, vectorstore ou agendamento.\",\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"Você trabalha na Tech4.ai e é um especialista em encaminhar uma pergunta do usuário para um vectorstore, pesquisa na web ou agendamento de reunião. \\n\n",
    "O vectorstore contém documentos relacionados a perguntas comuns apenas sobre a empresa (Tech4.ai), tais como missão, visão, valores, cultura, programas internos, políticas \\n\n",
    "de trabalho remoto, horários, etc. Use o vectorstore para perguntas sobre esses tópicos. \\n\n",
    "Para perguntas sobre as ferramentas Github, Vscode, Jira e discord use a pesquisa na web. \\n\n",
    "Para perguntas sobre agendamento de reuniões use o agendamento.\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n",
      "datasource='vectorstore'\n",
      "datasource='vectorstore'\n",
      "datasource='agendamento'\n",
      "datasource='agendamento'\n",
      "datasource='websearch'\n",
      "datasource='websearch'\n",
      "datasource='websearch'\n"
     ]
    }
   ],
   "source": [
    "print(question_router.invoke({\"question\": \"Qual a missão da Tech4.ai?\"}))\n",
    "print(question_router.invoke({\"question\": \"Quero saber os valores da empresa\"}))\n",
    "print(question_router.invoke({\"question\": \"Quero saber o que esperar de um dia de trabalho na Tech4.ai\"}))\n",
    "print(question_router.invoke({\"question\": \"gostaria de agendar uma reunião com o time de desenvolvimento\"}))\n",
    "print(question_router.invoke({\"question\": \"Como posso marcar uma reunião com o time de desenvolvimento?\"}))\n",
    "print(question_router.invoke({\"question\": \"Preciso de ajuda com o Github\"}))\n",
    "print(question_router.invoke({\"question\": \"Como posso usar o Vscode?\"}))\n",
    "print(question_router.invoke({\"question\": \"Como posso usar o Jira?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# Load PDF document\n",
    "pdf_path = \"Base.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the combined documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "\"\"\" \n",
    "(VOLTAR SE SOBRAR TEMPO!)\n",
    "O text_splitter não pega as imagens no final do pdf (Valores da Empresa). Tentativa de extrair texto das imagens:\n",
    "# Extract text from images\n",
    "image_paths = [\"image1.PNG\", \"image2.PNG\", \"image3.PNG\", \"image4.PNG\", \"image5.PNG\", \"image6.PNG\"]\n",
    "image_texts = [extract_text_from_image(image_path) for image_path in image_paths]\n",
    "\n",
    "doc_splitsIMAGES = text_splitter.split_documents(text_splitter.create_documents(image_texts)) \n",
    "\"\"\"\n",
    "\n",
    "# Load an open-source embedding model from Hugging Face\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Add to vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"Empresa\",\n",
    "    embedding=embedding_function,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Usar para deletar vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore = Chroma(collection_name=\"Empresa\")\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    " \n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Pontuação binária para verificar a relevância nos documentos utilizados.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documentos são relevantes para a pergunta, 'sim' or 'não'\")\n",
    "\n",
    "# LLM with function call \n",
    "structured_llm_grader_docs = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"Você é um avaliador que avalia a relevância de um documento recuperado para uma pergunta do usuário. \\n\n",
    " Se o documento contiver palavra(s)-chave ou significado semântico relacionado à pergunta, classifique-o como relevante. \\n\n",
    " Dê uma pontuação binária \"sim\" ou \"não\" para indicar se o documento é relevante para a pergunta.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader_relevance = grade_prompt | structured_llm_grader_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Você é um assistente para tarefas de resposta a perguntas. Use as seguintes partes do contexto recuperado para responder à pergunta. \\n\n",
    "      Se você não souber a resposta, apenas diga que não sabe. Sempre responda em português do Brasil.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    ")\n",
    " \n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='sim'\n",
      "Os programas internos da empresa mencionados são:\n",
      "\n",
      "1. Star Performers: reconhece os funcionários de alto desempenho na empresa.\n",
      "2. Monthly Highlights: destaca as atividades realizadas por cada departamento no mês passado.\n",
      "3. Team Talk: é um podcast apresentado por funcionários da empresa, discutindo tópicos relevantes e compartilhando insights.\n",
      "4. Network Sessions: promove o networking entre os funcionários em torno de temas pré-definidos.\n",
      "5. Learning Moments: apresenta palestras e workshops com especialistas externos para promover o aprendizado contínuo.\n",
      "6. Book Club: é um programa de leitura onde os funcionários compartilham conhecimentos e aprendem uns com os outros.\n",
      "7. Talent Development: visa o desenvolvimento comportamental dos funcionários.\n",
      "8. Engagement Projects: são projetos criados para aumentar o engajamento dos funcionários, incluindo jogos e atividades de produtividade.\n",
      "9. Referral Program: oferece uma recompensa aos funcionários que indicam candidatos bem-sucedidos para vagas na empresa.\n",
      "10. Innovation Lab: é um espaço para a criação de projetos que utilizam Inteligência Artificial.\n",
      "11. Education Support: oferece incentivos para cursos, treinamentos e livros.\n",
      "12. Travel Benefits: é um programa de benefícios de viagem que oferece descontos significativos e um orçamento para viagens, pago pela empresa.\n",
      "13. Team Integration: é um evento de integração semestral para todos os times da empresa.\n"
     ]
    }
   ],
   "source": [
    "question = \"Quais são os programas internos da empresa?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader_relevance.invoke({\"question\": question, \"document\": doc_txt}))\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination Grader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='sim')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Pontuação binária para alucinação presente na resposta obtida.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Não considere a possibilidade de chamar APIs externas para obter informações adicionais. A resposta é apoiada pelos fatos, 'sim' ou 'não'.\")\n",
    " \n",
    "# LLM with function call \n",
    "structured_llm_grader_hallucination = llm.with_structured_output(GradeHallucinations)\n",
    " \n",
    "# Prompt \n",
    "system = \"\"\"Você é um avaliador que avalia se uma resposta gerada por LLM é apoiada por um conjunto de fatos recuperados. \\n \n",
    "     Restrinja-se a dar uma pontuação binária, seja \"sim\" ou \"não\". Se a resposta for apoiada ou parcialmente apoiada pelo conjunto de fatos, considere-a um sim. \\n\n",
    "    Não considere a chamada de APIs externas para obter informações adicionais.\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "]\n",
    ")\n",
    "  \n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader_hallucination\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Grader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='sim')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Pontuação binária para avaliar se a resposta responde a pergunta.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Responde responde a pergunta, 'sim' ou 'não'\")\n",
    "\n",
    "# LLM with function call \n",
    "structured_llm_grader_answer = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"Você é um avaliador que avalia se uma resposta aborda/resolve uma pergunta \\n \n",
    "     Dê uma pontuação binária 'sim' ou 'não'. 'Sim' significa que a resposta resolve a pergunta.\"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader_answer\n",
    "answer_grader.invoke({\"question\": question,\"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web-Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.callbacks import wandb_tracing_enabled\n",
    "\n",
    "from langchain_google_calendar_tools.utils import build_resource_service, get_oauth_credentials\n",
    "from langchain_google_calendar_tools.tools.create_new_event.tool import CreateNewEvent\n",
    "from langchain_google_calendar_tools.tools.list_events.tool import ListEvents\n",
    "from langchain_google_calendar_tools.tools.update_exist_event.tool import UpdateExistEvent\n",
    "\n",
    "from langchain_google_calendar_tools.helper_tools.get_current_datetime import GetCurrentDatetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "browser_path = \"/usr/bin/firefox\"\n",
    "webbrowser.register('firefox', None, webbrowser.BackgroundBrowser(browser_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=326746794011-tk5t81lnatfpl17qfv35n0atc8mrj0gc.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A47117%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcalendar.events&state=Hu3ujIuHBLNwhoc3OUFZTwsz8SyPCg&access_type=offline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/firefox: 12: xdg-settings: not found\n",
      "2024/07/07 14:14:54.628113 cmd_run.go:1138: WARNING: cannot start document portal: dial unix /run/user/1000/bus: connect: no such file or directory\n",
      "[483020, Main Thread] WARNING: Settings portal not found: Could not connect: No such file or directory: 'glib warning', file /build/firefox/parts/firefox/build/toolkit/xre/nsSigHandlers.cpp:187\n",
      "\n",
      "(firefox:483020): Gdk-WARNING **: 14:14:55.834: Settings portal not found: Could not connect: No such file or directory\n",
      "[Parent 483020, Main Thread] WARNING: Failed to mkdir /home/matgaldino/snap/firefox/4483/.config/ibus/bus: Not a directory: 'glib warning', file /build/firefox/parts/firefox/build/toolkit/xre/nsSigHandlers.cpp:187\n",
      "\n",
      "(firefox:483020): IBUS-WARNING **: 14:14:56.770: Failed to mkdir /home/matgaldino/snap/firefox/4483/.config/ibus/bus: Not a directory\n"
     ]
    }
   ],
   "source": [
    "credentials = get_oauth_credentials(\n",
    "    client_secrets_file=\"credentials.json\",\n",
    ")\n",
    "\n",
    "api_resource = build_resource_service(credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matgaldino/TECHLAB/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(\n",
    "    tools=[\n",
    "        ListEvents(api_resource=api_resource),\n",
    "        CreateNewEvent(api_resource=api_resource),\n",
    "        UpdateExistEvent(api_resource=api_resource),\n",
    "        GetCurrentDatetime(),\n",
    "    ],\n",
    "    llm = ChatGroq(temperature=0, model=\"llama3-70b-8192\"),\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use wandb_tracing_enabled() to trace the agent's output.\n",
    "\n",
    "with wandb_tracing_enabled():\n",
    "    output = agent.run(\"Create new event on 2023-11-14 at 10:00 with summary 'test', return html link and event summary.\")\n",
    "\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents \n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    web_search : str\n",
    "    documents : List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE from Vector Store DB---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE Answer---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader_relevance.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"sim\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "    \n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH. Append to vector store db---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    if documents is not None:\n",
    "        documents.append(web_results)\n",
    "    else:\n",
    "        documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "### Edges\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG \n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})   \n",
    "    if source.datasource == 'websearch':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    elif source.datasource == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"sim\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"sim\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search) # web search # key: action to do\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # generatae\n",
    "\n",
    "workflow.add_edge(\"websearch\", \"generate\") #start -> end of node\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "    },\n",
    ")\n",
    " \n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\", # start: node\n",
    "    decide_to_generate, # defined function\n",
    "    {\n",
    "        \"websearch\": \"websearch\", #returns of the function\n",
    "        \"generate\": \"generate\",   #returns of the function\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\", # start: node\n",
    "    grade_generation_v_documents_and_question, # defined function\n",
    "    {\n",
    "        \"not supported\": \"generate\", #returns of the function\n",
    "        \"useful\": END,               #returns of the function\n",
    "        \"not useful\": \"websearch\",   #returns of the function\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE from Vector Store DB---\n",
      "'Finished running: retrieve:'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "'Finished running: grade_documents:'\n",
      "---GENERATE Answer---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "'Finished running: generate:'\n",
      "('Os programas internos da empresa mencionados são:\\n'\n",
      " '\\n'\n",
      " '1. Star Performers: reconhece os funcionários de alto desempenho na '\n",
      " 'empresa.\\n'\n",
      " '2. Monthly Highlights: destaca as atividades realizadas por cada '\n",
      " 'departamento no mês passado.\\n'\n",
      " '3. Team Talk: é um podcast apresentado por funcionários da empresa, '\n",
      " 'discutindo tópicos relevantes e compartilhando insights.\\n'\n",
      " '4. Network Sessions: promove o networking entre os funcionários em torno de '\n",
      " 'temas pré-definidos.\\n'\n",
      " '5. Learning Moments: apresenta palestras e workshops com especialistas '\n",
      " 'externos para promover o aprendizado contínuo.\\n'\n",
      " '6. Book Club: é um programa de leitura onde os funcionários compartilham '\n",
      " 'conhecimentos e aprendem uns com os outros.\\n'\n",
      " '7. Talent Development: visa o desenvolvimento comportamental dos '\n",
      " 'funcionários.\\n'\n",
      " '8. Engagement Projects: são projetos criados para aumentar o engajamento dos '\n",
      " 'funcionários, incluindo jogos e atividades de produtividade.\\n'\n",
      " '9. Referral Program: oferece uma recompensa aos funcionários que indicam '\n",
      " 'candidatos bem-sucedidos para vagas na empresa.\\n'\n",
      " '10. Innovation Lab: é um espaço para a criação de projetos que utilizam '\n",
      " 'Inteligência Artificial.\\n'\n",
      " '11. Education Support: oferece incentivos para cursos, treinamentos e '\n",
      " 'livros.\\n'\n",
      " '12. Travel Benefits: é um programa de benefícios de viagem que oferece '\n",
      " 'descontos significativos e um orçamento para viagens, pago pela empresa.\\n'\n",
      " '13. Team Integration: é um evento de integração semestral para todos os '\n",
      " 'times da empresa.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "inputs = {\"question\": \"Quais são os programas internos da empresa?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Finished running: {key}:\")\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
