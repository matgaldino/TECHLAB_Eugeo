{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolher as funções disponíveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Encaminhe uma consulta de usuário para o datasource mais relevante.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"websearch\", \"agendamento\"] = Field(\n",
    "        ...,\n",
    "        description=\"Dada uma pergunta do usuário, escolha encaminhá-la para websearch, vectorstore ou agendamento.\",\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"llama3-70b-8192\",\n",
    ")\n",
    "structured_llm_router = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"Você trabalha na Tech4.ai e é um especialista em encaminhar uma pergunta do usuário para um vectorstore, pesquisa na web ou agendamento de reunião. \\n\n",
    "O vectorstore contém documentos relacionados a perguntas comuns apenas sobre a empresa (Tech4.ai), tais como missão, visão, valores, cultura, programas internos, políticas \\n\n",
    "de trabalho remoto, horários, etc. Use o vectorstore para perguntas sobre esses tópicos. \\n\n",
    "Para perguntas sobre as ferramentas Github, Vscode, Jira e discord use a pesquisa na web. \\n\n",
    "Para perguntas sobre agendamento de reuniões use o agendamento.\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n",
      "datasource='vectorstore'\n",
      "datasource='vectorstore'\n",
      "datasource='agendamento'\n",
      "datasource='agendamento'\n",
      "datasource='websearch'\n",
      "datasource='websearch'\n",
      "datasource='websearch'\n"
     ]
    }
   ],
   "source": [
    "print(question_router.invoke({\"question\": \"Qual a missão da Tech4.ai?\"}))\n",
    "print(question_router.invoke({\"question\": \"Quero saber os valores da empresa\"}))\n",
    "print(question_router.invoke({\"question\": \"Quero saber o que esperar de um dia de trabalho na Tech4.ai\"}))\n",
    "print(question_router.invoke({\"question\": \"gostaria de agendar uma reunião com o time de desenvolvimento\"}))\n",
    "print(question_router.invoke({\"question\": \"Como posso marcar uma reunião com o time de desenvolvimento?\"}))\n",
    "print(question_router.invoke({\"question\": \"Preciso de ajuda com o Github\"}))\n",
    "print(question_router.invoke({\"question\": \"Como posso usar o Vscode?\"}))\n",
    "print(question_router.invoke({\"question\": \"Como posso usar o Jira?\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# Load PDF document\n",
    "pdf_path = \"Base.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the combined documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "\"\"\" \n",
    "(VOLTAR SE SOBRAR TEMPO!)\n",
    "O text_splitter não pega as imagens no final do pdf (Valores da Empresa). Tentativa de extrair texto das imagens:\n",
    "# Extract text from images\n",
    "image_paths = [\"image1.PNG\", \"image2.PNG\", \"image3.PNG\", \"image4.PNG\", \"image5.PNG\", \"image6.PNG\"]\n",
    "image_texts = [extract_text_from_image(image_path) for image_path in image_paths]\n",
    "\n",
    "doc_splitsIMAGES = text_splitter.split_documents(text_splitter.create_documents(image_texts)) \n",
    "\"\"\"\n",
    "\n",
    "# Load an open-source embedding model from Hugging Face\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Add to vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"Empresa\",\n",
    "    embedding=embedding_function,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Usar para deletar vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore = Chroma(collection_name=\"Empresa\")\n",
    "vectorstore.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    " \n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Pontuação binária para verificar a relevância nos documentos utilizados.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documentos são relevantes para a pergunta, 'sim' or 'não'\")\n",
    "\n",
    "# LLM with function call \n",
    "structured_llm_grader_docs = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"Você é um avaliador que avalia a relevância de um documento recuperado para uma pergunta do usuário. \\n\n",
    " Se o documento contiver palavra(s)-chave ou significado semântico relacionado à pergunta, classifique-o como relevante. \\n\n",
    " Dê uma pontuação binária \"sim\" ou \"não\" para indicar se o documento é relevante para a pergunta.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader_relevance = grade_prompt | structured_llm_grader_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Você é um assistente para tarefas de resposta a perguntas. Use as seguintes partes do contexto recuperado para responder à pergunta. \\n\n",
    "      Se você não souber a resposta, apenas diga que não sabe. Sempre responda em português do Brasil.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    ")\n",
    " \n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='sim'\n",
      "Sim, Travel Benefits é um programa de benefícios de viagem que oferece descontos significativos e um orçamento para viagens, pago pela empresa.\n"
     ]
    }
   ],
   "source": [
    "question = \"Travel benefits é um programa interno?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader_relevance.invoke({\"question\": question, \"document\": doc_txt}))\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination Grader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='sim')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Pontuação binária para alucinação presente na resposta obtida.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Não considere a possibilidade de chamar APIs externas para obter informações adicionais. A resposta é apoiada pelos fatos, 'sim' ou 'não'.\")\n",
    " \n",
    "# LLM with function call \n",
    "structured_llm_grader_hallucination = llm.with_structured_output(GradeHallucinations)\n",
    " \n",
    "# Prompt \n",
    "system = \"\"\"Você é um avaliador que avalia se uma resposta gerada por LLM é apoiada por um conjunto de fatos recuperados. \\n \n",
    "     Restrinja-se a dar uma pontuação binária, seja \"sim\" ou \"não\". Se a resposta for apoiada ou parcialmente apoiada pelo conjunto de fatos, considere-a um sim. \\n\n",
    "    Não considere a chamada de APIs externas para obter informações adicionais.\"\"\"\n",
    "\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "]\n",
    ")\n",
    "  \n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader_hallucination\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Grader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Pontuação binária para avaliar se a resposta responde a pergunta.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Responde responde a pergunta, 'sim' ou 'não'\")\n",
    "\n",
    "# LLM with function call \n",
    "structured_llm_grader_answer = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"Você é um avaliador que avalia se uma resposta aborda/resolve uma pergunta \\n \n",
    "     Dê uma pontuação binária 'sim' ou 'não'. 'Sim' significa que a resposta resolve a pergunta.\"\"\"\n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader_answer\n",
    "answer_grader.invoke({\"question\": question,\"generation\": generation})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
